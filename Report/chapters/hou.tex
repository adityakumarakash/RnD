\chapter{Higher Order Unification}

\emph{Unification} \footnote{parts of this chapter have been adopted from Ankit Gupta's MTP Report}, in computer science and logic, is an algorithmic process of solving equations between symbolic expressions. Depending on which expressions (also called terms) are allowed to occur in an equation set (also called \emph{unification problem}), and which expressions are considered equal, several frameworks of unification are distinguished. If \emph{higher-order variables}, that is, variables representing functions, are allowed in an expression, the process is called \emph{higher-order unification}, otherwise \emph{first-order unification}. If a solution is required to make both sides of each equation literally equal, the process is called \emph{syntactical unification}, otherwise \emph{semantical}, or \emph{equational unification}.

\section{Motivation}
Unification, especially higher-order unification, is a technique which is fundamental to the concept of program synthesis. Unification is what links program synthesis to theorem-proving. We had seen earlier in section 2.1, an example of program synthesis (for $max$ function of 2 numbers) where the starting point was purely a logical specification. However, in the process of trying to prove the goal (which is essentially a theorem), we found out the required function for calculating maximum. In essence \cite{Manna92fundamentalsof}, to construct a program meeting a given specification, we prove the existence of an object meeting the specified conditions. However, the proof is restricted to be sufficiently constructive, in the sense that, in establishing the existence of the desired output, the proof is forced to indicate a computational method for finding it. That method becomes the basis for a program that can be extracted from the proof. \\\\
Program synthesis, as we see from a functional programming perspective, is nothing but function computation. We are in essence trying to find a program (function) that satisfies a given (possibly logical) specification. 
For this we need to find the values of unknown variables in an equation, some of which would be functions. Higher-order unification thus becomes crucial.

\section{Illustration}
One of the most important steps during a derivation is to instantiate the unknown variables. For example, suppose we are at the following position in the derivation
\begin{center}
$x$ $+$ \emph{maximum(map sum(inits xs))} $=$ $?f$ $x$ ($?g$ $xs$) 
\end{center}
\begin{center}

\begin{adjustbox}{valign=c}
\begin{forest}
[+
   [$x$]
      [\emph{maximum}
			[\emph{map}
				[\emph{sum}]
				[\emph{inits}
					[$xs$]					
				]			
			]      
      ]
]
\end{forest}
\end{adjustbox}\qquad
$=$
\qquad
\begin{adjustbox}{valign=c}
\begin{forest}
[$?f$
	[$x$]
	[$?g$
		[$xs$]	
	]
]
\end{forest}
\end{adjustbox}

\end{center}
We want to find ``values'' of $?f$ and $?g$ (pattern variables) , such that the above equation gets ``solved''. In other words, we seek legitimate ``substitutions'' for $?f$ and $?g$, which ``unifies'' the terms on lhs and rhs. The following table shows the possible values for the unknown variables. We assume that we have not fixed any type of $?g$ (and hence of $?f$ ). We also assume that $x$ and $xs$ are bound variables, and cannot occur freely in substitutions for the pattern variables.

\begin{center}
\begin{tabular}{ | c | c | c | } 
\hline
 & \emph{\textbf{?f}} & \emph{\textbf{?g}} \\
\hline
 1. & \emph{$\lambda$a.$\lambda$b. a + b} & \emph{$\lambda$y. maximum (map sum (inits y))} \\
\hline
 2. & \emph{$\lambda$a.$\lambda$b. a + maximum b} &  \emph{$\lambda$y. map sum (inits y)}\\
\hline
 3. & \emph{$\lambda$a.$\lambda$b. a + maximum (map sum b)} & \emph{$\lambda$y. (inits y)} \\
\hline
 4. & \emph{$\lambda$a.$\lambda$b. a + maximum (map sum (inits b))} & \emph{$\lambda$y. y} \\
\hline

\end{tabular}
\end{center}
\pagebreak
It should be noted that, here we are considering variables which can be instantiated to $\lambda$-terms, and not just the ground terms; and equality is with respect to $\alpha \beta \eta$\- equivalence.
Hence this problem goes beyond the scope of the standard unification algorithm. Owing to this complexity, unlike first order unification, there may not be a canonical choice of the substitution, as shown by the following example.\\
For $0 =$ $?f$ $?x$, we can have,

\begin{center}
\begin{tabular}{ | c | c | c | } 
\hline
  & \emph{\textbf{?f}} & \emph{\textbf{?g}} \\
\hline
  1. & \emph{$\lambda$a.a} & \emph{0} \\
\hline
  2. & \emph{$\lambda$a.0} & \emph{-} \\
\hline
  3. & \emph{$\lambda$g.g 0} & \emph{$\lambda$a.a} \\
\hline
  4. & \emph{$\lambda$g.g (g 0)} & \emph{$\lambda$a.a} \\
\hline
  & ... & ... \\
\hline
\end{tabular}
\end{center}

\noindent
All these matches are incomparable i.e. none of them is a substitution instance of other. It has been shown that the problem of higher order unification is undecidable (Goldfarb \cite{goldfarb1981undecidability}). However, Huet and Lang have given an algorithm \cite{huet1978proving} for a restricted version of this problem, where we are interested only in terms upto \emph{second order}. This is basically a condition on types: a base type (for example \code{int} ) is first order. The order of a derived type is calculated by adding one to the order of the argument type and taking the maximum of this value and the order of the result type. So for example \code{int} $\rightarrow$ \code{bool} is second order. The order of a term is simply the order of its type. This simple restriction guarantees that there are only a finite number of incomparable matches. In the above example, the matchings which are allowed are 1 and 2.\\\\
In the context of functional program derivation and transformation, this restriction is not reasonable, because functions are treated as first class values and can be passed as arguments to other functions. For example, \emph{foldr} :: ($\alpha \rightarrow \beta \rightarrow \beta $) $\rightarrow \beta \rightarrow [\alpha] \rightarrow \beta$ is a third order term and routinely used in fusion laws. To overcome this problem, Oege
de Moor and Ganesh Sittampalam have given an algorithm for higher order unification \cite{de2001higher} in the context of program derivation, which finds more matchings than the standard second order matching algorithm by Huet and Lang. However, they do not claim that the algorithm gives all the third order matchings. Ankit Gupta has studied and implemented this algorithm in Coq. In this chapter, we formally define the problem of higher order unification and a solution/substitution to the hou problem. We further define formally the restricted hou problem specification that Ankit deals with, and the property of the solution obtained.

\section{The Problem Specification}
\subsection{Definitions}
\noindent
An expression is an untyped $\lambda$-term, i.e., a variable , a constant, a $\lambda$-abstraction or an application. A variable can be either a \emph{bound} (local) variable or a \emph{pattern} (free) variable (indicated by a preceding ?).

\begin{center}
\emph{E := c $|$ x $|$ $?$p $|$ ($E_1$ $E_2$ ) $|$ ($\lambda$x.E)}
\end{center}

\noindent
The notion of ``equality'' of two expressions is also more complicated as compared to first order matching. Equality is considered modulo
\begin{itemize}
\item $\alpha$ - \emph{conversion} : Renaming of bound variables. Hence, the following equality holds
\begin{center}
	\emph{($\lambda$x. $\lambda$y.a x (b x y)) = ($\lambda$y. $\lambda$z.a y (b y z))}
\end{center}
\item $\eta$ - \emph{conversion} : Elimination of superfluous arguments. The $\eta$-conversion rule states that \emph{($\lambda$x.E x)} can be written as E , provided x is not free in E. For example,
\begin{center}
	\emph{($\lambda$x.$\lambda$y.a x y) = ($\lambda$x.a x) = a}
\end{center}
But, the following is not true
\begin{center}
\emph{($\lambda$x.$\lambda$y.a y x) = a}
\end{center}
\item $\beta$ - \emph{conversion} : Substitution of arguments for parameters. An expression of the form \emph{($\lambda$x.$E_1$) $E_2$} (called as $\beta$-redex)is converted to \emph{$E_1$ (x := $E_2$ )}. The application of this rule in a left-to-right direction is known as $\beta$ reduction.
\end{itemize}

An expression is said to be \emph{normal} if it does not contain any $\eta$-redex or $\beta$-redex as a subexpression. An expression is \emph{closed} if all the variables it contains are bound by an enclosing $\lambda$-abstraction. Here, although we are dealing with untyped $\lambda$-terms, it can be easily extended to the typed setting, by representing types explicitly in the expressions (as in second order $\lambda$-calculus).\\\\
A \emph{substitution} is a total function mapping pattern variables to expressions. We shall specify a substitution by listing those assignments to variables that are not the identity. For instance,

\begin{center}
\emph{$\varphi$ = $\{$ ?p := (a r), ?q := ($\lambda$y.b y x) $\}$}
\end{center}

\noindent
A substitution is said to be \emph{normal} if all expressions in its range are normal, and closed if any variables that it changes are mapped to closed expressions. \emph{Composition} of substitutions $\varphi$ and $\psi$ is defined by first applying $\psi$ and then $\varphi$.

\begin{center}
\emph{($\varphi$ $\circ$ $\psi$) E = $\varphi$ ($\psi$ E)}
\end{center}

\noindent
A \emph{rule} is a pair of expressions, written as \emph{(P $\rightarrow$ T )}, where $P$ does not contain any $\eta$-redex, and $T$ is normal, with all variables in $T$ being local variables, i.e. they occur under an enclosing $\lambda$-abstraction. The matching process starts off with $T$ closed, but because it proceeds by structural recursion it can generate new rules which do not have $T$ closed. In such a rule, a variable is still regarded as being local if it occurred under an enclosing $\lambda$-abstraction in the original rule. We call $P$ the \emph{pattern} and $T$ the \emph{term} of the rule.

\subsection{Parallel $\beta$-reduction}
We know define the key operation involved in the specification of the (restricted) higher order unification problem. The function \emph{step} performs a bottom-up traversal of an expression, applying $\beta$-reduction wherever possible. Formally,

\begin{equation*}
\begin{split}
step \: c \: &= \: c \\
step \: x &= \: x \\
step \: ?p &= \: ?p \\
step \: (\lambda x.E) &= \: \lambda x.(step \: E) \\
step \: (E_1 \: E_2 ) \: &= \: case \: E_1' \: of \\
& \qquad \lambda x.B \: \rightarrow \: B \: (x \: := \: E_2') \\
& \qquad \rightarrow \: (E_1' \: E_2' \:) \\
& \qquad where \: E_1' \: = \: step \: E_1 \\
& \qquad E_2' \: = \: step E_2
\end{split}
\end{equation*}

\vspace{1em}
\noindent
\emph{step} proceeds by recursion on the structure of terms and hence always terminates. It is not quite the same as the operation that applies $\beta$ reduction exhaustively, until no more redexes remain (generally called as $\beta$-\emph{normalise}). For example,

\vspace{1em}
\emph{step}
\qquad
\begin{adjustbox}{valign=c}
\begin{forest}
[$@$
	[$@$
		[$\lambda x$
			[$\lambda y$
				[$@$
					[$@$
						[$a$]
						[$@$
							[$x$]
							[$b$]						
						]
					]
					[$y$]				
				]			
			]		
		]
		[$\lambda z$
			[$@$
				[$@$
					[$c$]
					[$z$]				
				]
				[$z$]			
			]		
		]
	]
	[$d$]
]
\end{forest}
\end{adjustbox}\qquad
=
\qquad
\begin{adjustbox}{valign=c}
\begin{forest}
[$@$
	[$@$
		[$a$]
		[$@$
			[$\lambda z$
				[$@$
					[$@$
						[$c$]
						[$z$]					
					]
					[$z$]				
				]			
			]
			[$b$]		
		]	
	]
	[$d$]
]
\end{forest}
\end{adjustbox}

\noindent
In a certain sense, step represents an approximation of $\beta$-normalise : if $\beta$-\emph{normalise} $E$ exists, then there exists some positive integer n such that step$^n$ $E$ = $\beta$-\emph{normalise} $E$. However, $\beta$-\emph{normalise} $E$ does not always terminate.

\subsection{Restricted HOU problem specification}
A substitution $\phi$ is said to be \emph{pertinent} to a rule ($P$ $\rightarrow$ $T$ ) if all variables it changes are contained in P . Similarly, a substitution is pertinent to a set of rules if all variables it changes are contained in the pattern of one of the rules. A rule ($P$ $\rightarrow$ $T$ ) is \emph{satisfied} by a normal substitution $\phi$ if

\begin{align*}
\eta -normalise \: (step (\phi \: P )) \: &= \: T
\end{align*}

\vspace{1em}
\noindent
The substitution $\phi$ is then said to be a \emph{one-step match}. Note that we take equality in the above modulo $\alpha$-renaming. A normal substitution satisfies a set of rules if it satisfies all elements of that set.\\\\
A \emph{match set} M of a set of rules Xs is a set of non-redundant normal substitutions which satisfy Xs (as per the one step match) . The notion of a one-step match contrasts with that of a general match in that it restricts the notion of equality somewhat; a normal substitution $\phi$ is said to be a general match if

\begin{align*}
\eta -normalise \: (\beta -normalise \: (\phi \: P )) \: = \: T
\end{align*}

\vspace{1em}
\noindent
It is because of the above restricted specification, that we do not get all possible matches (which may be infinite in number). In the earlier example of $\{?f\: ?x \rightarrow 0 \}$, the following is a feasible match set :

\begin{align*}
\{
\{ ?f \: := \: (\lambda \: y.0) \} ,
\{ ?f \: := \: (\lambda \: y.y) , \: ?x \: := \: 0 \}
\}
\end{align*}

\vspace{1em}
\noindent
Clearly,

\begin{align*}
step \: ((\lambda \: y.0) \: ?x) \: &= \: 0\\
step \: ((\lambda \: y.y) \: 0) \: &= \: 0
\end{align*}

\vspace{1em}
\noindent
Note that, for the substitution $\{ ?f \: := \: (\lambda g.g \: 0) , \: ?x \: := \: (\lambda \: y.y) \}$, we have

\begin{align*}
\beta -normalise \: ((\lambda \: g.g \: 0) \: (\lambda \: y.y)) \: &= \:0
\end{align*}

\vspace{1em}
\noindent
However,

\begin{align*}
step \: ((\lambda \: g.g \: 0) \: (\lambda \: y.y)) \: &= \: ((\lambda \: y.y) \: 0) \: \neq \: 0
\end{align*}

\vspace{1em}
\noindent
Hence this substitution and all other similar substitutions mentioned earlier do not appear in the match set. Now for computing the match set of a given set of rules, we use the algorithm proposed by de Moor and Sittampalam \cite{de2001higher}. We do not describe the details of the algorithm in this report, and insist the reader to go through \cite{de2001higher} if inquisitive. In essence we wish to make the point that, improvements in unification algorithms is one dimension along which program synthesis can be developed. Since unification is the key technique that introduces constructivism into theorem-proving, the greater the space of functions we search for, the more are the theorems that can be proved.
